import torch
import open_clip
from PIL import Image

# 1ï¸âƒ£ CLIP ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')
tokenizer = open_clip.get_tokenizer('ViT-B-32')

# 2ï¸âƒ£ ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
image = preprocess(Image.open("dog.jpg")).unsqueeze(0)

# 3ï¸âƒ£ ì¿¼ë¦¬ ë¬¸ì¥ ì—¬ëŸ¬ê°œ ìƒì„± (query variants)
queries = [
    "a dog running on the grass",
    "a brown dog playing outdoors",
    "a dog sprinting in the field",
    "a running puppy outside",
    "brown dog running fast"
]

# 4ï¸âƒ£ ê° ë¬¸ì¥ì„ Text Encoder í†µê³¼ì‹œì¼œ ì„ë² ë”©
text_embeds = []
with torch.no_grad():
    for q in queries:
        tok = tokenizer([q])
        e_i = model.encode_text(tok)
        e_i = e_i / e_i.norm(dim=-1, keepdim=True)
        text_embeds.append(e_i)

E = torch.cat(text_embeds, dim=0)  # (5, 512)

# 5ï¸âƒ£ ì´ë¯¸ì§€ ì„ë² ë”©
with torch.no_grad():
    image_emb = model.encode_image(image)
    image_emb = image_emb / image_emb.norm(dim=-1, keepdim=True)

# 6ï¸âƒ£ Baseline: ë‹¨ìˆœ í‰ê· 
fusion_mean = E.mean(dim=0)
fusion_mean = fusion_mean / fusion_mean.norm()

# 7ï¸âƒ£ AQF: Adaptive Query Fusion (ìœ ì‚¬ë„ ê¸°ë°˜ softmax ê°€ì¤‘)
sims = (E @ image_emb.T).squeeze(1)              # ê° queryë³„ ìœ ì‚¬ë„
weights = torch.softmax(sims / 0.05, dim=0)      # ì˜¨ë„ 0.05
fusion_aqf = (weights.unsqueeze(1) * E).sum(dim=0)
fusion_aqf = fusion_aqf / fusion_aqf.norm()

# 8ï¸âƒ£ ê²°ê³¼ ë¹„êµ
sim_mean = (fusion_mean @ image_emb.T).item()
sim_aqf = (fusion_aqf @ image_emb.T).item()

print("ğŸ§© Queryë³„ ìœ ì‚¬ë„:", [round(s.item(), 3) for s in sims])
print("âš™ï¸  Softmax ê°€ì¤‘ì¹˜:", [round(w.item(), 3) for w in weights])
print(f"\nBaseline (mean fusion): {sim_mean:.3f}")
print(f"AQF (adaptive fusion):  {sim_aqf:.3f}")
